# Imbalanced Learning
*This workshop is part of the "Machine Learning in R" graduate course held at University of Münster, School of Business and Economics (winter term 2020/21).* :mortar_board:

**Time:** Mo, 2020-11-23  
**Location:** Zoom  
**Presenter:** Simon Schölzel, M.Sc.  
**Slides:** https://simonschoe.github.io/imbalanced-learning/

<a href="https://www.wiwi.uni-muenster.de/"><img src="https://www.wiwi.uni-muenster.de/fakultaet/sites/all/themes/wwucd/assets/images/logos/secondary_wiwi_aacsb_german.jpg" alt="fb4-logo" height="45"></a> <a href="https://www.wiwi.uni-muenster.de/ctrl/aktuelles"><img src="https://www.wiwi.uni-muenster.de/ctrl/sites/all/themes/wwucd/assets/images/logos/berenslogo5.jpg" alt="ftb-logo" height="45"></a> <a href="https://www.wiwi.uni-muenster.de/iff2/de/news"><img src="https://www.wiwi.uni-muenster.de/iff2/sites/all/themes/wwucd/assets/images/logos/logo_iff2_en2.jpg" alt="ipb-logo" height="45"></a>


## Contents

This 1 hour workshop serves as a gentle introduction to the field of imbalanced learning. Participants will not only learn about the peculiarities and implications of working with imbalanced data sets, but also how to address class imbalance within the typical machine learning pipeline.

More specifically, after this workshop participants will
- be able to identify an imbalanced data set and know about its implications for modeling,
- carry a toolbox of techniques for addressing class imbalance at various stages in your machine learning pipeline (e.g., data collection, resampling, model estimation or model evaluation),
- have internalized basic (random under- and oversampling) and more advanced techniques of resampling (SMOTE, Borderline SMOTE, NearMiss),
- know how to distinguish alternative routes to handling class imbalance, such as imbalanced learning or cost-sensitive learning.


## Agenda

**1 Learning Objectives**

**2 Introduction to Imbalanced Learning**

**3 Techniques for Addressing Class Imbalance**
>3.1 Sampling Strategies
>> 3.1.1 Random Oversampling  
3.1.2 Synthetic Minority Over-Sampling Technique (SMOTE)  
3.1.3 Borderline SMOTE  
3.1.4 Random Undersampling  
3.1.5 Informed NearMiss Undersampling 

>3.2 Case Weighting  
3.3 Excursus: Evaluation of Classification Models  
3.4 Tinkering with Classification Cutoffs

**4 Cost-Sensitive Learning**
